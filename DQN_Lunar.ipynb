{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLdipIOWf1Rx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6694b0f3-fa70-419e-9fa8-1ef6dbd3af29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gymnasium[box2d]\n",
            "  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 KB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium[box2d]) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium[box2d]) (1.22.4)\n",
            "Collecting farama-notifications>=0.0.1\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium[box2d]) (6.1.0)\n",
            "Collecting jax-jumpy>=1.0.0\n",
            "  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
            "Collecting pygame==2.1.3\n",
            "  Downloading pygame-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting swig==4.*\n",
            "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting box2d-py==2.3.5\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 KB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gymnasium[box2d]) (3.15.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "Installing collected packages: swig, farama-notifications, box2d-py, pygame, jax-jumpy, gymnasium\n",
            "  Running setup.py install for box2d-py ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: box2d-py was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\u001b[0m\u001b[33m\n",
            "\u001b[0m  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.3.0\n",
            "    Uninstalling pygame-2.3.0:\n",
            "      Successfully uninstalled pygame-2.3.0\n",
            "Successfully installed box2d-py-2.3.5 farama-notifications-0.0.4 gymnasium-0.28.1 jax-jumpy-1.0.0 pygame-2.1.3 swig-4.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ECEN 743: Reinforcement Learning\n",
        "Deep Q-Learning\n",
        "Code tested using\n",
        "\t1. gymnasium 0.27.1\n",
        "\t2. box2d-py  2.3.5\n",
        "\t3. pytorch   2.0.0\n",
        "\t4. Python    3.9.12\n",
        "1 & 2 can be installed using pip install gymnasium[box2d]\n",
        "\n",
        "General Instructions\n",
        "1. This code consists of TODO blocks, read them carefully and complete each of the blocks\n",
        "2. Type your code between the following lines\n",
        "\t\t\t###### TYPE YOUR CODE HERE ######\n",
        "\t\t\t#################################\n",
        "3. The default hyperparameters should be able to solve LunarLander-v2\n",
        "4. You do not need to modify the rest of the code for this assignment, feel free to do so if needed.\n",
        "\n",
        "\"\"\"\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import numpy as np\n",
        "from collections import deque, namedtuple\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "\n",
        "class ExperienceReplay:\n",
        "\t\"\"\" \n",
        "\tBased on the Replay Buffer implementation of TD3 \n",
        "\tReference: https://github.com/sfujim/TD3/blob/master/utils.py\n",
        "\t\"\"\"\n",
        "\tdef __init__(self, state_dim, action_dim,max_size,batch_size,gpu_index=0):\n",
        "\t\tself.max_size = max_size\n",
        "\t\tself.ptr = 0\n",
        "\t\tself.size = 0\n",
        "\t\tself.state = np.zeros((max_size, state_dim))\n",
        "\t\tself.action = np.zeros((max_size, action_dim))\n",
        "\t\tself.next_state = np.zeros((max_size, state_dim))\n",
        "\t\tself.reward = np.zeros((max_size, 1))\n",
        "\t\tself.done = np.zeros((max_size, 1))\t\t\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.device = torch.device('cuda', index=gpu_index) if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "\n",
        "\tdef add(self, state, action,reward,next_state, done):\n",
        "\t\tself.state[self.ptr] = state\n",
        "\t\tself.action[self.ptr] = action\n",
        "\t\tself.next_state[self.ptr] = next_state\n",
        "\t\tself.reward[self.ptr] = reward\n",
        "\t\tself.done[self.ptr] = done\n",
        "\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
        "\t\tself.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "\tdef sample(self):\n",
        "\t\tind = np.random.randint(0, self.size, size=self.batch_size)\n",
        "\n",
        "\t\treturn (\n",
        "\t\t\ttorch.FloatTensor(self.state[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.action[ind]).long().to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.next_state[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.done[ind]).to(self.device)\n",
        "\t\t)\n",
        "\n",
        "\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "\t\"\"\"\n",
        "\tQ Network: designed to take state as input and give out Q values of actions as output\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, state_dim, action_dim):\n",
        "\t\t\"\"\"\n",
        "\t\t\tstate_dim (int): state dimenssion\n",
        "\t\t\taction_dim (int): action dimenssion\n",
        "\t\t\"\"\"\n",
        "\t\tsuper(QNetwork, self).__init__()\n",
        "\t\tself.l1 = nn.Linear(state_dim, 64)\n",
        "\t\tself.l2 = nn.Linear(64, 64)\n",
        "\t\tself.l3 = nn.Linear(64, action_dim)\n",
        "\t\t\n",
        "\tdef forward(self, state):\n",
        "\t\tq = F.relu(self.l1(state))\n",
        "\t\tq = F.relu(self.l2(q))\n",
        "\t\treturn self.l3(q)\n",
        "\n",
        "\n",
        "\n",
        "class DQNAgent():\n",
        "\n",
        "\tdef __init__(self,\n",
        "\t state_dim, \n",
        "\t action_dim,\n",
        "\t discount=0.99,\n",
        "\t tau=1e-3,\n",
        "\t lr=5e-4, #\n",
        "\t update_freq=4,\n",
        "\t max_size=int(1e5),\n",
        "\t batch_size=64,\n",
        "\t gpu_index=0\n",
        "\t ):\n",
        "\t\t\"\"\"\n",
        "\t\t\tstate_size (int): dimension of each state\n",
        "\t\t\taction_size (int): dimension of each action\n",
        "\t\t\tdiscount (float): discount factor\n",
        "\t\t\ttau (float): used to update q-target\n",
        "\t\t\tlr (float): learning rate\n",
        "\t\t\tupdate_freq (int): update frequency of target network\n",
        "\t\t\tmax_size (int): experience replay buffer size\n",
        "\t\t\tbatch_size (int): training batch size\n",
        "\t\t\tgpu_index (int): GPU used for training\n",
        "\t\t\"\"\"\n",
        "\t\tself.state_dim = state_dim\n",
        "\t\tself.action_dim = action_dim\n",
        "\t\tself.discount = discount\n",
        "\t\tself.tau = tau\n",
        "\t\tself.lr = lr\n",
        "\t\tself.update_freq = update_freq\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.device = torch.device('cuda', index=gpu_index) if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "\n",
        "\t\t# Setting up the NNs\n",
        "\t\tself.Q = QNetwork(state_dim, action_dim).to(self.device)\n",
        "\t\tself.Q_target = QNetwork(state_dim, action_dim).to(self.device)\n",
        "\t\tself.optimizer = optim.Adam(self.Q.parameters(), lr=self.lr)\n",
        "\n",
        "\t\t# Experience Replay Buffer\n",
        "\t\tself.memory = ExperienceReplay(state_dim,1,max_size,self.batch_size,gpu_index)\n",
        "\t\t\n",
        "\t\tself.t_train = 0\n",
        "\t\n",
        "\tdef step(self, state, action, reward, next_state, done):\n",
        "\t\t\"\"\"\n",
        "\t\t1. Adds (s,a,r,s') to the experience replay buffer, and updates the networks\n",
        "\t\t2. Learns when the experience replay buffer has enough samples\n",
        "\t\t3. Updates target netowork\n",
        "\t\t\"\"\"\n",
        "\t\tself.memory.add(state, action, reward, next_state, done)\t   \n",
        "\t\tself.t_train += 1 \n",
        "\t\t\t\t\t\n",
        "\t\tif self.memory.size > self.batch_size:\n",
        "\t\t\texp = self.memory.sample()\n",
        "\t\t\tself.learn(exp, self.discount) #To be implemented\n",
        "\t\t\n",
        "\t\tif (self.t_train % self.update_freq) == 0:\n",
        "\t\t\tself.target_update(self.Q, self.Q_target, self.tau) #To be implemented \n",
        "\n",
        "\tdef select_action(self, state, epsilon=0.):\n",
        "\t\t\"\"\"\n",
        "\t\tTODO: Complete this block to select action using epsilon greedy exploration \n",
        "\t\tstrategy\n",
        "\t\tInput: state, epsilon\n",
        "\t\tReturn: Action\n",
        "\t\tReturn Type: int\t\n",
        "\t\t\"\"\"\n",
        "\t\t###### TYPE YOUR CODE HERE ######\n",
        "\t\tself.Q.eval() \n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tstate = torch.FloatTensor(state).to(self.device)\n",
        "\t\t\tq_value = self.Q(state)\n",
        "\t\t\tbest_act = np.argmax(q_value.numpy(force=True))\n",
        "\t\tif np.random.random() < epsilon:\n",
        "\t\t\tchosen_act =  np.random.randint(0,self.action_dim)\n",
        "\t\telse:\n",
        "\t\t\tchosen_act =  best_act\n",
        "\t\tself.Q.train() \n",
        "\t\treturn chosen_act\n",
        "\t\t################################# \n",
        "\n",
        "\tdef learn(self, experiences, discount):\n",
        "\t\t\"\"\"\n",
        "\t\tTODO: Complete this block to update the Q-Network using the target network\n",
        "\t\t1. Compute target using  self.Q_target ( target = r + discount * max_b [Q_target(s,b)] )\n",
        "\t\t2. Compute Q(s,a) using self.Q\n",
        "\t\t3. Compute MSE loss between step 1 and step 2\n",
        "\t\t4. Update your network\n",
        "\t\tInput: experiences consisting of states,actions,rewards,next_states and discount factor\n",
        "\t\tReturn: None\n",
        "\t\t\"\"\" \t\t\n",
        "\t\tstates, actions, rewards, next_states, dones = experiences\n",
        "\t\t###### TYPE YOUR CODE HERE ######\n",
        "\t\tself.Q_target.eval()\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tq_val_target = self.Q_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "\t\ttarget = rewards + discount * q_val_target * (1-dones)\n",
        "\t\tself.Q.train()\n",
        "\t\tq_val = self.Q(states).gather(1, actions)\n",
        "\t\tself.optimizer.zero_grad()\n",
        "\t\tmseloss = F.mse_loss(q_val,target)\n",
        "\t\tmseloss.backward()\n",
        "\t\tself.optimizer.step()\n",
        "\t\t#################################\n",
        "                \n",
        "\tdef target_update(self, Q, Q_target, tau):\n",
        "\t\t\"\"\"\n",
        "\t\tTODO: Update the target network parameters (param_target) using current Q parameters (param_Q)\n",
        "\t\tPerform the update using tau, this ensures that we do not change the target network drastically\n",
        "\t\t1. param_target = tau * param_Q + (1 - tau) * param_target\n",
        "\t\tInput: Q,Q_target,tau\n",
        "\t\tReturn: None\n",
        "\t\t\"\"\" \n",
        "\t\t###### TYPE YOUR CODE HERE ######\n",
        "\t\tfor target, Q in zip(Q_target.parameters(), Q.parameters()):\n",
        "\t\t\ttarget.data.copy_(tau*Q.data + (1.0-tau)*target.data)\n",
        "\t\t#################################\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\tparser = argparse.ArgumentParser()\n",
        "\tparser.add_argument(\"--env\", default=\"LunarLander-v2\")          # Gymnasium environment name\n",
        "\tparser.add_argument(\"--seed\", default=0, type=int)              # sets Gym, PyTorch and Numpy seeds\n",
        "\tparser.add_argument(\"--n-episodes\", default=2000, type=int)     # maximum number of training episodes\n",
        "\tparser.add_argument(\"--batch-size\", default=64, type=int)       # training batch size\n",
        "\tparser.add_argument(\"--discount\", default=0.99)                 # discount factor\n",
        "\tparser.add_argument(\"--lr\", default=5e-4)                       # learning rate\n",
        "\tparser.add_argument(\"--tau\", default=0.001)                     # soft update of target network\n",
        "\tparser.add_argument(\"--max-size\", default=int(1e5),type=int)    # experience replay buffer length\n",
        "\tparser.add_argument(\"--update-freq\", default=4, type=int)       # update frequency of target network\n",
        "\tparser.add_argument(\"--gpu-index\", default=0,type=int)\t\t    # GPU index\n",
        "\tparser.add_argument(\"--max-esp-len\", default=1000, type=int)    # maximum time of an episode\n",
        "\t#exploration strategy\n",
        "\tparser.add_argument(\"--epsilon-start\", default=1)               # start value of epsilon\n",
        "\tparser.add_argument(\"--epsilon-end\", default=0.01)              # end value of epsilon\n",
        "\tparser.add_argument(\"--epsilon-decay\", default=0.995)           # decay value of epsilon\n",
        "\targs = parser.parse_args(args=[]) ##\n",
        "\n",
        "\t# making the environment\t\n",
        "\tenv = gym.make(args.env)\n",
        "\tenv = gym.make(args.env,render_mode=\"rgb_array_list\")\n",
        "\tenv = gym.wrappers.RecordVideo(env=env,video_folder='./video')\n",
        "\n",
        "\t#setting seeds\n",
        "\ttorch.manual_seed(args.seed)\n",
        "\tnp.random.seed(args.seed)\n",
        "\trandom.seed(args.seed)\n",
        "\n",
        "\tstate_dim = env.observation_space.shape[0]\n",
        "\taction_dim = env.action_space.n\n",
        "\n",
        "\tkwargs = {\n",
        "\t\t\"state_dim\":state_dim,\n",
        "\t\t\"action_dim\":action_dim,\n",
        "\t\t\"discount\":args.discount,\n",
        "\t \t\"tau\":args.tau,\n",
        "\t \t\"lr\":args.lr,\n",
        "\t \t\"update_freq\":args.update_freq,\n",
        "\t \t\"max_size\":args.max_size,\n",
        "\t \t\"batch_size\":args.batch_size,\n",
        "\t \t\"gpu_index\":args.gpu_index\n",
        "\t}\t\n",
        "\tlearner = DQNAgent(**kwargs) #Creating the DQN learning agent\n",
        "\n",
        "\twindow = deque(maxlen=100)\n",
        "\tsmoothened_cumulative_reward_history = []\n",
        "\tconsistency_count = 0\n",
        "\tepsilon = args.epsilon_start\n",
        "\tfor e in range(args.n_episodes):\n",
        "\t\tstate, _ = env.reset(seed=args.seed)\n",
        "\t\tcurr_reward = 0\n",
        "\t\tfor t in range(args.max_esp_len):\n",
        "\t\t\taction = learner.select_action(state,epsilon) #To be implemented\n",
        "\t\t\tn_state,reward,terminated,truncated,_ = env.step(action)\n",
        "\t\t\tdone = terminated or truncated \n",
        "\t\t\tlearner.step(state,action,reward,n_state,done) #To be implemented\n",
        "\t\t\tstate = n_state\n",
        "\t\t\tcurr_reward += reward\n",
        "\t\t\tif done:\n",
        "\t\t\t\tbreak\n",
        "\t\twindow.append(curr_reward)\n",
        "\n",
        "\t\t\"\"\"\"\n",
        "\t\tTODO: Write code for decaying the exploration rate using args.epsilon_decay\n",
        "\t\tand args.epsilon_end. Note that epsilon has been initialized to args.epsilon_start  \n",
        "\t\t1. You are encouraged to try new methods\n",
        "\t\t\"\"\"\n",
        "\t\t###### TYPE YOUR CODE HERE ######\n",
        "\t\tepsilon = max(args.epsilon_end, epsilon * args.epsilon_decay)\n",
        "\t\t#################################\t\n",
        "\t\t\n",
        "\t\tif e % 100 == 0:\n",
        "\t\t\tprint('Episode Number {} Average Episodic Reward (over 100 episodes): {:.2f}'.format(e, np.mean(window)))\n",
        "\t\t\n",
        "\t\t\"\"\"\"\n",
        "\t\tTODO: Write code for\n",
        "\t\t1. Logging and plotting\n",
        "\t\t2. Rendering the trained agent \n",
        "\t\t\"\"\"\n",
        "\t\t###### TYPE YOUR CODE HERE ######\n",
        "\t\tsmoothened_cumulative_reward_history.append(np.mean(indow))\n",
        "\n",
        "\t\tif np.mean(window)>=200.:\n",
        "\t\t\tconsistency_count += 1\n",
        "\t\t\tif consistency_count == 100:\n",
        "\t\t\t\ttorch.save(learner.Q.state_dict(),'./model.ckpt')\n",
        "\t\t\t\tbreak\n",
        "\t\telse:\n",
        "\t\t\tconsistency_count = 0\n",
        "\t\t\n",
        "\t# now that training has finished, we can plot cumulative reward vs episodes\n",
        "\tpickle.dump(reward_history,open('cum_rwe','wb'))\n",
        "\tplt.subplots_adjust(bottom=0.2)\n",
        "\tplt.plot(reward_history)\n",
        "\tplt.xlabel('TrainingEpisodes')\n",
        "\tplt.ylabel('Episodic Cumulative reward')\n",
        "\tlearner.Q.load_state_dict(torch.load('./model.ckpt'))\n",
        "\tlearner.Q.eval()\n",
        "\twith torch.no_grad():\n",
        "\t\tfor i in range(1):\n",
        "\t\t\tstate,_ = env.reset()\n",
        "\t\t\tcum_reward = 0\n",
        "\t\t\twhile True:\n",
        "\t\t\t\taction = np.argmax(learner.Q(torch.FloatTensor(state).to(learner.device)).numpy(force=True))\n",
        "\t\t\t\tenv.render()\n",
        "\t\t\t\tstate,reward,done,_,_ = env.step(action)\n",
        "\t\t\t\tcum_reward += reward\n",
        "\t\t\t\tif done:\n",
        "\t\t\t\t\tbreak\n",
        "\t\t\tprint(f'Reward for this episode is {cum_reward}')\n",
        "\t#env.close()\n",
        "\t#################################"
      ],
      "metadata": {
        "id": "Rwgi36gyNXOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dqLCwHUw8xg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9m 44s - Lunar Lander r1\n",
        "# 9m 40s - Lunar Lander r2\n",
        "# 23m 48s - Lunar Lander r3 - Video\n",
        "# 26m 52s - Acrobot - Video"
      ],
      "metadata": {
        "id": "z-h4fmPCNXit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "071pD_RegFWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-dof6m_SgFd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VL2_B9NUZ1QX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sG51QDlFZ1TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iero6BnZZ1Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fu45vEALZ1bm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}